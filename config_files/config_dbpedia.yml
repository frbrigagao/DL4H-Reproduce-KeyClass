# DATASET CONFIGURATION
dataset: dbpedia                        # DBPedia dataset
data_path: ../original_data/            # Path to dataset
n_classes: 14                           # Number of classes (paper p.9)
max_num: 250000                         # Maximum samples per class to use
problem_type: single_label              # Single-label classification problem

# Class descriptions for DBPedia
target_00: company
target_01: school, university
target_02: artist
target_03: athlete
target_04: politics
target_05: transportation
target_06: building
target_07: river, mountain, lake
target_08: village
target_09: animal
target_10: plant, tree
target_11: album
target_12: film
target_13: novel, publication, book

# ENCODER CONFIGURATION
base_encoder: paraphrase-mpnet-base-v2  # MPNET encoder mentioned in paper p.9
use_custom_encoder: false               # Using default MPNET, not BlueBERT
normalize_embeddings: false             # Whether to normalize embeddings

# KEYWORD EXTRACTION
topk: 15                                # Top-k labeling functions per class (paper p.9)
min_df: 0.001                           # Document frequency threshold (paper p.8)
ngram_range: !!python/tuple             # Extract 1-3 grams as keywords
- 1
- 3

# LABEL MODEL CONFIGURATION
label_model: data_programming           # Using data programming or majority_vote
label_model_lr: 0.01                    # Learning rate for label model
label_model_n_epochs: 100               # Number of epochs for label model
use_noise_aware_loss: true              # Using noise-aware loss for weak labels

# DOWNSTREAM CLASSIFIER ARCHITECTURE
h_sizes:                                # Neural network architecture (paper p.11)
- 768                                   # Input size (MPNET embedding dimension, paper p.9)
- 256                                   # Hidden layer 1
- 64                                    # Hidden layer 2
- 14                                    # Output layer (14 classes for DBPedia)
activation: torch.nn.LeakyReLU()        # LeakyReLU activation (paper p.11)
criterion: torch.nn.CrossEntropyLoss(reduction='none')  # Loss function (paper p.11)

# DOWNSTREAM CLASSIFIER TRAINING
end_model_batch_size: 128               # Batch size for training (paper p.11)
end_model_epochs: 20                    # Maximum training epochs (paper p.11)
end_model_lr: 1e-3                      # Learning rate (paper p.11)
end_model_patience: 2                   # Early stopping patience (paper p.11)
end_model_weight_decay: 1e-4            # Weight decay for regularization

# SELF-TRAINING CONFIGURATION
self_train_batch_size: 8                # Batch size for self-training
self_train_lr: 1e-6                     # Learning rate for self-training
self_train_patience: 3                  # Early stopping patience for self-training
self_train_thresh: 1-5e-3               # Agreement threshold (1-0.005 = 0.995)
self_train_weight_decay: 1e-4           # Weight decay for self-training
q_update_interval: 50                   # Target distribution update frequency

# EVALUATION CONFIGURATION
average: weighted                       # Type of averaging for metrics
n_bootstrap: 100                        # Bootstrap samples for confidence intervals

# INFRASTRUCTURE & LOGGING PARAMETERS
device: cuda                            # Hardware accelerator to use
n_jobs: 10                              # Number of parallel jobs
show_progress_bar: true                 # Display progress during training
log_path: ../logs/                      # Log file path
results_path: ../results/dbpedia/       # Path for results