activation: torch.nn.LeakyReLU() # Mentioned in the paper (page 11)
average: weighted
base_encoder: paraphrase-mpnet-base-v2 # Mentioned in the paper (page 9)
criterion: torch.nn.CrossEntropyLoss(reduction='none') # Mentioned in the paper (page 11)
data_path: ../original_data/
dataset: dbpedia
device: cuda
end_model_batch_size: 128 # The paper used a batch size of 128 (page 11)
end_model_epochs: 20  # The paper trained the model with a max of 20 epochs (page 11)
end_model_lr: 1e-3    # The paper used a learning rate of 0.001 (page 11)
end_model_patience: 2 # The paper used a patience of 2 (page 11)
end_model_weight_decay: 1e-4
h_sizes: # 4 Layers mentioned in the paper (page 11)
- 768 # Mentioned in the paper (page 9)
- 256
- 64
- 14
label_model: data_programming
label_model_lr: 0.01
label_model_n_epochs: 100
log_file: ../logs/config_dbpedia.log
max_num: 250000 # Max number of data points per class
min_df: 0.001 # Minimum document frequency threshold for word inclusion
model_path: ../models/dbpedia/
n_bootstrap: 100
n_classes: 14
n_jobs: 10 
ngram_range: !!python/tuple
- 1
- 3
normalize_embeddings: false
preds_path: ../results/dbpedia/
q_update_interval: 50
results_path: ../results/dbpedia/
self_train_batch_size: 8  
self_train_lr: 1e-6       
self_train_patience: 3
self_train_thresh: 1-5e-3
self_train_weight_decay: 1e-4
show_progress_bar: true
target_00: company
target_01: school, university
target_02: artist
target_03: athlete
target_04: politics
target_05: transportation
target_06: building
target_07: river, mountain, lake
target_08: village
target_09: animal
target_10: plant, tree
target_11: album
target_12: film
target_13: novel, publication, book
topk: 15 # Mentioned in the paper (page 9)
use_custom_encoder: false
use_noise_aware_loss: true
